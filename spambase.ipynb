{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adcf47c",
   "metadata": {},
   "source": [
    "# Rebecca Weiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40b9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99657dd7",
   "metadata": {},
   "source": [
    "# I will implement the random forest algorithm from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d5bfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data \n",
    "cols = ['word_freq_make:',\n",
    "    'word_freq_address:',\n",
    " 'word_freq_all:',\n",
    " 'word_freq_3d:',\n",
    " 'word_freq_our:',\n",
    " 'word_freq_over:',\n",
    " 'word_freq_remove:',\n",
    " 'word_freq_internet:',\n",
    " 'word_freq_order:',\n",
    " 'word_freq_mail:',\n",
    " 'word_freq_receive:',\n",
    " 'word_freq_will:',\n",
    " 'word_freq_people:',\n",
    " 'word_freq_report:',\n",
    " 'word_freq_addresses:',\n",
    " 'word_freq_free:',\n",
    " 'word_freq_business:',\n",
    " 'word_freq_email:',\n",
    " 'word_freq_you:',\n",
    " 'word_freq_credit:',\n",
    " 'word_freq_your:',\n",
    " 'word_freq_font:',\n",
    " 'word_freq_000:',\n",
    " 'word_freq_money:',\n",
    " 'word_freq_hp:',\n",
    " 'word_freq_hpl:',\n",
    " 'word_freq_george:',\n",
    " 'word_freq_650:',\n",
    " 'word_freq_lab:',\n",
    " 'word_freq_labs:',\n",
    " 'word_freq_telnet:',\n",
    " 'word_freq_857:',\n",
    " 'word_freq_data:',\n",
    " 'word_freq_415:',\n",
    " 'word_freq_85:',\n",
    " 'word_freq_technology:',\n",
    " 'word_freq_1999:',\n",
    " 'word_freq_parts:',\n",
    " 'word_freq_pm:',\n",
    " 'word_freq_direct:',\n",
    " 'word_freq_cs:',\n",
    " 'word_freq_meeting:',\n",
    " 'word_freq_original:',\n",
    " 'word_freq_project:',\n",
    " 'word_freq_re:',\n",
    " 'word_freq_edu:',\n",
    " 'word_freq_table:',\n",
    " 'word_freq_conference:',\n",
    " 'char_freq_;:',\n",
    " 'char_freq_(:',\n",
    " 'char_freq_[:',\n",
    " 'char_freq_!:',\n",
    " 'char_freq_$:',\n",
    " 'char_freq_#:',\n",
    " 'capital_run_length_average:',\n",
    " 'capital_run_length_longest:',\n",
    " 'capital_run_length_total:', 'spam']\n",
    "\n",
    "df = pd.read_csv('spambase.data', names=cols)\n",
    "df.head(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0eb2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (3680, 57) (3680,)\n",
      "Test data (921, 57) (921,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into testing and training data\n",
    "X = df.drop(['spam'], axis=1)\n",
    "y = df['spam']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(\"Training data\", X_train.shape, y_train.shape)\n",
    "print(\"Test data\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758fe0c",
   "metadata": {},
   "source": [
    "## I will start by creating a bootstrap sample of size 1000 and then select a subset of p columns. I will vary the value of p, and report the p that results in the lowest cross-validation error. Then, I will train a decision tree classifier on the bootstrap sample by setting the depth to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed10e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find best subset p, using average CV\n",
    "n, p = X_train.shape\n",
    "scores = {i: [] for i in range(1, 57)}\n",
    "\n",
    "for i in range(1000): \n",
    "    random_p = random.randint(1, 56)\n",
    "    X_idx = random.sample([x for x in range(1, n)], 1000)\n",
    "    Y_idx = random.sample([x for x in range(1, p)], random_p)\n",
    "    \n",
    "    subsample_X = X_train.iloc[X_idx, Y_idx]\n",
    "    subsample_Y = y_train.iloc[X_idx]\n",
    "    \n",
    "    dectree = DecisionTreeClassifier()\n",
    "    model = dectree.fit(subsample_X, subsample_Y)\n",
    "    cv_scores = cross_val_score(model, subsample_X, subsample_Y, cv=10)\n",
    "    scores[random_p].append(np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb40bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of p = 52, that results in the best average cross validation score = 0.879\n"
     ]
    }
   ],
   "source": [
    "subset_p = []\n",
    "cv_avg = []\n",
    "\n",
    "for key, value in scores.items():\n",
    "    subset_p.append(key)\n",
    "    cv_avg.append(np.mean(np.abs(value)))\n",
    "\n",
    "\n",
    "cv_df = pd.DataFrame({'subset_p': subset_p, 'Average CV Score': cv_avg}).sort_values('Average CV Score', ascending=False)\n",
    "\n",
    "\n",
    "# get value of p with best average cv\n",
    "val = cv_df.iloc[0,:]\n",
    "print(\"Subset of p = {:.0f}, that results in the best average cross validation score = {:.3f}\".format(val[0], val[1]))\n",
    "# cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6755c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.154\n"
     ]
    }
   ],
   "source": [
    "best_p = int(val[0])\n",
    "best_p\n",
    "\n",
    "#tree at best subset of p\n",
    "dectree = DecisionTreeClassifier(max_depth=6)\n",
    "model = dectree.fit(X_train[:best_p], y_train[:best_p])\n",
    "ypred = model.predict(X_test[:best_p])\n",
    "\n",
    "mse = mean_squared_error(y_test[:best_p], ypred)\n",
    "print('MSE = {:.3f}'.format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85f8cc",
   "metadata": {},
   "source": [
    "## Now I will repeat these calculations to generate T ∈ {1, 50, 100, 150, 200, 300, 400} trees and evaluate on the training set.  I will combine the predictions from all trees and assign the final class based on a majority vote of the predictions of every tree. In case of ties, I will assign a class randomly among the ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8f4249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When T = 1, MSE = 0.073, Model Score = 0.71386\n",
      "When T = 50, MSE = 0.055, Model Score = 0.92255\n",
      "When T = 100, MSE = 0.055, Model Score = 0.92473\n",
      "When T = 150, MSE = 0.058, Model Score = 0.92446\n",
      "When T = 200, MSE = 0.059, Model Score = 0.92337\n",
      "When T = 300, MSE = 0.058, Model Score = 0.92527\n",
      "When T = 400, MSE = 0.059, Model Score = 0.92310\n"
     ]
    }
   ],
   "source": [
    "T =  [1, 50, 100, 150, 200, 300, 400]\n",
    "\n",
    "for num_trees in T:\n",
    "    dtree = BaggingClassifier(DecisionTreeClassifier(max_depth=6), n_estimators=num_trees, oob_score=True) \n",
    "    model = dtree.fit(X_train, y_train)\n",
    "    ypred = model.predict(X_test) # prediction for test set \n",
    "#     print(ypred)\n",
    "    mse = mean_squared_error(ypred, y_test)\n",
    "    score = model.oob_score_\n",
    "    print('When T = {}, MSE = {:.3f}, Model Score = {:.5f}'.format(num_trees, mse, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d9e99",
   "metadata": {},
   "source": [
    "When model score < 0.5, we will classify the label to be 0, thus not predicted to be spam. For all values of T, the score is close to 1, thus predicted to be spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706cef1",
   "metadata": {},
   "source": [
    "## Now I will report the training and test error, F1 score, and AUC by varying T in the range {1, 50, 100, 150, 200, 300, 400}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bb2f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When T = 1, Training Error = 0.077, Testing Error = 0.923, F1 Score = 0.924, AUC = 0.929\n",
      "\n",
      "When T = 50, Training Error = 0.055, Testing Error = 0.945, F1 Score = 0.945, AUC = 0.948\n",
      "\n",
      "When T = 100, Training Error = 0.053, Testing Error = 0.947, F1 Score = 0.947, AUC = 0.951\n",
      "\n",
      "When T = 150, Training Error = 0.060, Testing Error = 0.940, F1 Score = 0.941, AUC = 0.942\n",
      "\n",
      "When T = 200, Training Error = 0.060, Testing Error = 0.940, F1 Score = 0.941, AUC = 0.942\n",
      "\n",
      "When T = 300, Training Error = 0.059, Testing Error = 0.941, F1 Score = 0.942, AUC = 0.944\n",
      "\n",
      "When T = 400, Training Error = 0.059, Testing Error = 0.941, F1 Score = 0.942, AUC = 0.944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = [1, 50, 100, 150, 200, 300, 400]\n",
    "\n",
    "for num_trees in T:\n",
    "    dtree = BaggingClassifier(DecisionTreeClassifier(max_depth = 6), n_estimators=num_trees, random_state=123)\n",
    "    model = dtree.fit(X_train, y_train)\n",
    "    ypred = model.predict(X_test)\n",
    "    mse = mean_squared_error(ypred, y_test)\n",
    "    score = accuracy_score(ypred, y_test)\n",
    "    f1 = f1_score(ypred, y_test, average='weighted')\n",
    "    auc = roc_auc_score(ypred, y_test, average='weighted')\n",
    "    print('When T = {}, Training Error = {:.3f}, Testing Error = {:.3f}, F1 Score = {:.3f}, AUC = {:.3f}\\n'.format(num_trees, mse, score, f1, auc))\n",
    "    # print(classification_report(y_true=y_test, y_pred=ypred, target_names=['not spam', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cce5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.94245385450597 %\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=6)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "classification_report(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "roc_auc_score(y_pred, y_test, average='weighted')\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e70a0",
   "metadata": {},
   "source": [
    "## Now I will use RandomForestClassifer from sklearn.ensemble library to train a Random Forest algorithm with 10, 50, and 100 decision trees and report similar metrics on both the training and testing sets, and the top 10 features having the most influence on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c8522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When T = 10, MSE = 0.066\n",
      "When T = 50, MSE = 0.067\n",
      "When T = 100, MSE = 0.065\n",
      "\n",
      " Top 10 features and their Importance having the most influence on the model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>char_freq_$:</td>\n",
       "      <td>0.143483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>char_freq_!:</td>\n",
       "      <td>0.114414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word_freq_remove:</td>\n",
       "      <td>0.092214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>word_freq_free:</td>\n",
       "      <td>0.084814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>word_freq_your:</td>\n",
       "      <td>0.065117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>word_freq_hp:</td>\n",
       "      <td>0.059523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>capital_run_length_average:</td>\n",
       "      <td>0.059337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>capital_run_length_longest:</td>\n",
       "      <td>0.058074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>word_freq_money:</td>\n",
       "      <td>0.049315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word_freq_our:</td>\n",
       "      <td>0.033174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Features  Importance\n",
       "52                 char_freq_$:    0.143483\n",
       "51                 char_freq_!:    0.114414\n",
       "6             word_freq_remove:    0.092214\n",
       "15              word_freq_free:    0.084814\n",
       "20              word_freq_your:    0.065117\n",
       "24                word_freq_hp:    0.059523\n",
       "54  capital_run_length_average:    0.059337\n",
       "55  capital_run_length_longest:    0.058074\n",
       "23             word_freq_money:    0.049315\n",
       "4                word_freq_our:    0.033174"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "T = [10, 50, 100]\n",
    "\n",
    "for num_trees in T:\n",
    "    rf = RandomForestClassifier(max_depth=6, random_state=123, n_estimators=num_trees)\n",
    "    modelrf = rf.fit(X_train, y_train)\n",
    "    ypred = modelrf.predict(X_test)\n",
    "    mse = mean_squared_error(ypred, y_test)\n",
    "    print('When T = {}, MSE = {:.3f}'.format(num_trees, mse))\n",
    "\n",
    "# Create dataframe for top 10 features:\n",
    "top = pd.DataFrame({'Features': X_train.columns, 'Importance': modelrf.feature_importances_}, columns=['Features', 'Importance']).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n Top 10 features and their Importance having the most influence on the model:\")\n",
    "top[:10]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
